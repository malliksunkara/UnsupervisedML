{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:26:21.050189Z",
     "start_time": "2018-03-26T21:26:19.749574Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:26:36.492687Z",
     "start_time": "2018-03-26T21:26:21.847190Z"
    }
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', data_home='../Data/')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', data_home='../Data/')\n",
    "\n",
    "vectortype_train = TfidfVectorizer(stop_words='english')\n",
    "vectortype_test = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "news_vectored_result_train = vectortype_train.fit_transform(newsgroups_train.data)\n",
    "news_vectored_result_test = vectortype_test.fit_transform(newsgroups_test.data)\n",
    "\n",
    "news_train_target = newsgroups_train.target\n",
    "news_train_target_names = newsgroups_train.target_names\n",
    "news_test_target = newsgroups_test.target\n",
    "\n",
    "\n",
    "train_features = vectortype_train.get_feature_names()\n",
    "test_features = vectortype_test.get_feature_names()\n",
    "common_features = np.intersect1d(train_features, test_features)\n",
    "\n",
    "train_feature_final = np.searchsorted(train_features, common_features)\n",
    "news_vt_train = news_vectored_result_train[:,train_feature_final]\n",
    "\n",
    "test_feature_final = np.searchsorted(test_features, common_features)\n",
    "news_vt_test = news_vectored_result_test[:,test_feature_final]\n",
    "\n",
    "news_vt_train = np.array(news_vt_train.todense())\n",
    "news_vt_test = np.array(news_vt_test.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:26:39.247291Z",
     "start_time": "2018-03-26T21:26:39.243615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49600 20\n"
     ]
    }
   ],
   "source": [
    "col_size = news_vt_train.shape[1]\n",
    "class_size = len(np.unique(news_train_target))\n",
    "print(col_size, class_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:26:41.949976Z",
     "start_time": "2018-03-26T21:26:41.855099Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_steps = 300\n",
    "batch_size = 128\n",
    "display_step = 30\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 300 # 1st layer number of neurons\n",
    "n_hidden_2 = 300 # 2nd layer number of neurons\n",
    "num_input = col_size # MNIST data input (img shape: 28*28)\n",
    "num_classes = class_size # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "weights = {\n",
    "'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:26:44.622461Z",
     "start_time": "2018-03-26T21:26:44.619186Z"
    }
   },
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:27:24.383551Z",
     "start_time": "2018-03-26T21:27:24.287067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:27:52.402612Z",
     "start_time": "2018-03-26T21:27:52.399590Z"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle(matrix1, matrix2):\n",
    "    index = np.arange(np.shape(matrix1)[0])\n",
    "    np.random.shuffle(index)\n",
    "    return matrix1[index, :], matrix2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:27:55.168121Z",
     "start_time": "2018-03-26T21:27:55.145322Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatch(data, old, batch_size):\n",
    "        if old+batch_size <= data.shape[0]:\n",
    "            batch_x = data[old:old+batch_size]\n",
    "            y = target[old:old+batch_size]\n",
    "            #print(batch_x.shape, old, old + batch_size)\n",
    "            old += batch_size\n",
    "            batch_y = []\n",
    "            for item in y:\n",
    "                temp = np.zeros((20),dtype=float)\n",
    "                temp[item] = 1\n",
    "                batch_y.append(temp)\n",
    "            return batch_x, np.array(batch_y), old\n",
    "        else:\n",
    "            batch_x = data[old:old+batch_size]\n",
    "            y = target[old:old+batch_size]\n",
    "            t_batch_size = batch_size - batch_x.shape[0]\n",
    "            old = 0\n",
    "            new_batch_x = data[old:t_batch_size]\n",
    "            new_y = target[old:t_batch_size]\n",
    "            batch_x = np.concatenate((batch_x, new_batch_x), axis=0)\n",
    "            y = np.concatenate((y, new_y), axis=0)\n",
    "            #print('\\n\\n', batch_x.shape, old, old +t_batch_size)\n",
    "            old += t_batch_size\n",
    "            batch_y = []\n",
    "            for item in y:\n",
    "                temp = np.zeros((20),dtype=float)\n",
    "                temp[item] = 1\n",
    "                batch_y.append(temp)\n",
    "            return batch_x, np.array(batch_y), old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:28:07.610763Z",
     "start_time": "2018-03-26T21:27:57.793738Z"
    }
   },
   "outputs": [],
   "source": [
    "data, target = shuffle(news_vt_train, news_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:29:37.140148Z",
     "start_time": "2018-03-26T21:28:36.868732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "Step 1, Minibatch Loss= 736.3987, Training Accuracy= 0.570\n",
      "Step 30, Minibatch Loss= 86.0449, Training Accuracy= 0.906\n",
      "Step 60, Minibatch Loss= 35.8812, Training Accuracy= 0.977\n",
      "Step 90, Minibatch Loss= 7.6157, Training Accuracy= 0.984\n",
      "Step 120, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Step 150, Minibatch Loss= 0.5971, Training Accuracy= 0.992\n",
      "Step 180, Minibatch Loss= 2.5590, Training Accuracy= 0.992\n",
      "Step 210, Minibatch Loss= 5.4527, Training Accuracy= 0.984\n",
      "Step 240, Minibatch Loss= 11.5293, Training Accuracy= 0.992\n",
      "Step 270, Minibatch Loss= 2.8895, Training Accuracy= 0.992\n",
      "Step 300, Minibatch Loss= 5.2950, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "(128, 20)\n",
      "(7532, 20)\n",
      "Testing Accuracy: 0.746017\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    print('A')\n",
    "    sess.run(init)\n",
    "    print('B')\n",
    "    old=0\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y, old = getBatch(data, old, batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    batch_y = []\n",
    "    for item in news_test_target:\n",
    "        temp = np.zeros((20),dtype=float)\n",
    "        temp[item] = 1\n",
    "        batch_y.append(temp)\n",
    "    test_target_batch = np.array(batch_y)\n",
    "    print(\"Testing Accuracy:\", \n",
    "        sess.run(accuracy, feed_dict={X: news_vt_test,\n",
    "                                      Y: test_target_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T03:53:06.844017Z",
     "start_time": "2018-03-22T03:53:06.827542Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
